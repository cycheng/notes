# Contents
  * Kubernetes (k8s)
  * ASAP - real2sim2real
  * DeepSeek
    * v2
  * high-flyer

## Kubernetes (k8s)
* Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation.
* Virtualized deployment era:
  * Each VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware.
* Container deployment era:
  * Containers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications.
    Therefore, containers are considered lightweight.
  * ä¸ä¼ ç»Ÿçš„è™šæ‹Ÿæœºä¸åŒï¼Œå®¹å™¨å…±äº«ä¸»æœºæ“ä½œç³»ç»Ÿçš„å†…æ ¸ï¼Œå› æ­¤æ›´åŠ é«˜æ•ˆå’Œä¾¿æºã€‚ 

## ASAP - real2sim2real
https://hao.cnyes.com/post/133793

## DeepSeek
### DeepSeek-VL2 - 2024 Jun 19
* https://github.com/deepseek-ai/DeepSeek-VL2

### DeepSeek-V2
* https://github.com/deepseek-ai/DeepSeek-V2
* a strong Mixture-of-Experts (MoE) language model
* 236B total parameters, 21B are activated for each token,  context length of 128K tokens
* innovative architectures
  * Multi-head Latent Attention (MLA)
    * MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector
  * DeepSeekMoE
    * enables training strong models at an economical cost through sparse computation
  * v.s. DeepSeek 67B:
    * significantly stronger performance
    * saves 42.5% of training costs
    * reduces the KV cache by 93.3%
    * boosts the maximum generation throughput to 5.76 times
  * pretrain: on a high-quality and multi-source corpus consisting of 8.1T tokens
  * Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)
![image](https://github.com/user-attachments/assets/bde50e68-af54-4627-8dd2-239be143dbe6)

1. Introduction
* we introduce MLA, an attention mechanism equipped with low-rank key-value joint compression.
  Empirically, MLA achieves superior performance compared with MHA, significantly reduces the KV cache during inference, thus boosting the inference efficiency.
* For Feed-Forward Networks (FFNs), we follow the DeepSeekMoE architecture (Dai et al., 2024), which adopts
  fine-grained expert segmentation and shared expert isolation for higher potential in expert specialization 
* We construct a high-quality and multi-source pre-training corpus consisting of 8.1T tokens.
* DeepSeek-V2 Chat (SFT): we collect 1.5M conversational sessions, which encompass various domains such as math, code, 
  writing, reasoning, safety, and more, to perform Supervised Fine-Tuning (SFT)  
* DeepSeek-V2 Chat (RL): Finally, we employ Group Relative Policy Optimization (GRPO) to further align the model with
  human preference
![image](https://github.com/user-attachments/assets/44477fe4-c33f-4cce-9cc4-40c9f1257ed8)
* We evaluate DeepSeek-V2 on a wide range of benchmarks in English and Chinese
  * even with only 21B activated parameters, V2 is the strongest open-source MoE language model
  * compared with DeepSeek 67B, V2 saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the
    maximum generation throughput to 5.76 times
  * V2 Chat (RL) achieves
    - 38.9 length-controlled win rate on AlpacaEval 2.0 (Dubois et al., 2024),
    - 8.97 overall score on MT-Bench (Zheng et al., 2023), and
    - 7.91 overall score on AlignBench (Liu et al., 2023).
  * DeepSeek-V2-Lite
    - 15.7B parameters
    - 2.4B are activated for each token.

2. Architecture
  * For attention, we design MLA, which utilizes low-rank key-value joint compression to
    eliminate the bottleneck of inference-time key-value cache, thus supporting efficient inference.
  * For Feed-Forward Networks (FFNs), we adopt the DeepSeekMoE architecture (Dai et al., 2024), a
    high-performance MoE architecture that enables training strong models at an economical cost.

2.1. Multi-Head Latent Attention: Boosting Inference Efficiency

2.1.1. Preliminaries: Standard Multi-Head Attention

#### Multi-Head Attention (MHA) è§£é‡‹ by DeepSeek V3:
* https://chat.deepseek.com/a/chat/s/e579d7f5-60ad-4fa2-87c3-88b37221b0a2

Multi-Head Attention (MHA) æ˜¯ Transformer æ¨¡å‹ä¸­çš„æ ¸å¿ƒæ©Ÿåˆ¶ï¼Œç”¨æ–¼æ•æ‰è¼¸å…¥åºåˆ—ä¸­ä¸åŒä½ç½®çš„é—œä¿‚ã€‚ä»¥ä¸‹æ˜¯å…¶é—œéµé»ï¼š
* Self-Attention åŸºç¤
  * é€šéè¨ˆç®—è¼¸å…¥åºåˆ—ä¸­æ¯å€‹å…ƒç´ èˆ‡å…¶ä»–å…ƒç´ çš„ç›¸é—œæ€§ï¼Œç”ŸæˆåŠ æ¬Šè¡¨ç¤ºã€‚
  * è¼¸å…¥åºåˆ— $$X = (x_1, x_2, \dots, x_n)$$\
    é€šéç·šæ€§è®Šæ›å¾—åˆ°æŸ¥è©¢ï¼ˆQueryï¼‰ã€éµï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰å‘é‡ï¼š\
    $$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$
  * æ³¨æ„åŠ›å¾—åˆ†é€šéé»ç©è¨ˆç®—ï¼š\
    $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$\
    å…¶ä¸­ $$\( d_k \)$$ æ˜¯éµå‘é‡çš„ç¶­åº¦ã€‚

* Multi-Head Attention
  - é€šéä¸¦è¡Œè¨ˆç®—å¤šå€‹ Self-Attention é ­ï¼Œæ•æ‰ä¸åŒçš„ç‰¹å¾µè¡¨ç¤ºã€‚
  - æ¯å€‹é ­ç¨ç«‹è¨ˆç®—æ³¨æ„åŠ›ï¼š\
    $head_i = \text{Attention}(QW_{Q_i}, KW_{K_i}, VW_{V_i})$

  - æ‰€æœ‰é ­çš„è¼¸å‡ºæ‹¼æ¥å¾Œé€šéç·šæ€§è®Šæ›ï¼š\
    $\text{MHA}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W_O$\
    $å…¶ä¸­ \ h \ æ˜¯é ­æ•¸ï¼Œ\ W_O \ æ˜¯è¼¸å‡ºæ¬Šé‡çŸ©é™£ã€‚$

* å„ªé»
  - **ä¸¦è¡Œè¨ˆç®—**ï¼šå¤šå€‹é ­å¯ä»¥åŒæ™‚è™•ç†ä¸åŒç‰¹å¾µã€‚
  - **å¤šæ¨£åŒ–ç‰¹å¾µæ•æ‰**ï¼šæ¯å€‹é ­é—œæ³¨ä¸åŒçš„å­ç©ºé–“ï¼Œå¢å¼·æ¨¡å‹è¡¨é”èƒ½åŠ›ã€‚

* æ‡‰ç”¨
  - **Transformer**ï¼šMHA æ˜¯å…¶ç·¨ç¢¼å™¨å’Œè§£ç¢¼å™¨çš„æ ¸å¿ƒã€‚
  - **BERTã€GPT**ï¼šé€™äº›é è¨“ç·´æ¨¡å‹ä¹Ÿä¾è³´ MHA é€²è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚

* ç¸½çµ
  - **Multi-Head Attention** é€šéä¸¦è¡Œè¨ˆç®—å¤šå€‹ Self-Attention é ­ï¼Œå¢å¼·äº†æ¨¡å‹æ•æ‰å¤šæ¨£åŒ–ç‰¹å¾µçš„èƒ½åŠ›ï¼Œæ˜¯ Transformer æ¨¡å‹æˆåŠŸçš„é—œéµã€‚

#### Multi-Head Attention (MHA) è§£é‡‹ by This paper 2.1.1:

$\text{Let } d \text{ be the embedding dimension, }$\
$n_h \text{ be the number of attention heads, }$\
$d_h \text{ be the dimension per head, and }$\
$h_t \in \mathbb{R}^d \text{ be the attention input of the ğ‘¡-th token at an attention layer.}$\
Standard MHA first produces $q_t, k_t, v_t \in \mathbb{R}^{d_h n_h}$ through three matrices $W^Q, W^K, W^V \in \mathbb{R}^{d_h n_h \times d}$
![image](https://github.com/user-attachments/assets/812a3f6e-4157-41e6-a204-f28eec731fdf)

Then, $q_t, k_t, v_t$ will be sliced into $n_h$ heads for the multi-head attention computation:
![image](https://github.com/user-attachments/assets/37c9eb32-0668-4a36-9d21-b9e9730ba823)

* where $q_{t,i}, k_{t,i}, v_{t,i} \in \mathbb{R}^{d_h}$ denote the query, key, and value of the ğ‘–-th attention head, respectively;
* $W^o \in \mathbb{R}^{d \times d_h n_h}$ denotes the output projection matrix.
* During inference, all keys and values need to be cached to accelerate inference, so MHA needs to cache $2n_h d_h l$ elements for each token. 
* In model deployment, this heavy KV cache is a large bottleneck that limits the maximum batch size and sequence length.

æˆ‘çš„ç†è§£:
æ­¤è¡¨ä¾†è‡ª chatgpt 4o:
* https://chatgpt.com/share/682dc278-486c-800c-b0e5-6dd121f2dfdb
* 
| æ¦‚å¿µ               | æ„ç¾©               | ç›´è§€æ¯”å–»        |
| ---------------- | ---------------- | ----------- |
| Query            | æˆ‘è¦é—œæ³¨ä»€éº¼ï¼Ÿ          | ç™¼å•è€…         |
| Key              | æˆ‘èƒ½æä¾›ä»€éº¼è³‡è¨Šï¼Ÿ        | å€™é¸ç·šç´¢        |
| Value            | æˆ‘çš„å¯¦éš›å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ       | æœ€å¾Œå–ç”¨çš„ç­”æ¡ˆ     |
| Attention(Q,K,V) | æ‰¾å‡ºæœ€ relevant çš„å…§å®¹ | å•å°å•é¡Œã€æ‰¾åˆ°å°çš„è³‡æ–™ |

* æ¯å€‹ head é—œæ³¨ä¸åŒçš„è§’åº¦ (æå‡ºä¸åŒçš„å•é¡Œ)
  * æ¯å€‹ head æœ‰è‡ªå·±çš„ä¸€å¥—æŠ•å½±çŸ©é™£ï¼ˆQ/K/Vï¼‰ï¼Œæ‰€ä»¥å®ƒå¯ä»¥å¾ä¸åŒã€Œèªæ„å­ç©ºé–“ã€å»åˆ†æè³‡æ–™ã€‚
  * æœ‰çš„ head å°ˆæ³¨èªæ³•ï¼Œæœ‰çš„å°ˆæ³¨èªæ„ï¼Œæœ‰çš„é—œæ³¨ç›¸å°ä½ç½®ã€‚
* query: å°ä¸€å€‹ token æå‡ºæŸå€‹ head çš„å•é¡Œ
  * Query æ˜¯é€™å€‹ token åœ¨è©² head è£¡ç™¼å‡ºçš„ä¸€å€‹æŸ¥è©¢å‘é‡
  * è©¦åœ–å•ï¼šã€Œæˆ‘è©²é—œæ³¨èª°ï¼Ÿèª°èˆ‡æˆ‘æœ‰èªæ„ä¸Šçš„é€£çµï¼Ÿã€
* key: é€™å€‹ token å°æŸå€‹ head èƒ½æä¾›ä»€éº¼è¨Šæ¯
  * Key æ˜¯å…¶ä»– token å°æ‡‰çš„ã€Œè‡ªæˆ‘æè¿°ã€ï¼šæˆ‘æ˜¯èª°ï¼Ÿæˆ‘åœ¨é€™å€‹èªå¢ƒä¸­æœ‰ä»€éº¼è§’è‰²ï¼Ÿ
  * æ¯å€‹ token å°æ¯å€‹ head æœƒæœ‰ä¸åŒçš„ Keyï¼Œå°æ‡‰ä¸åŒçš„ã€Œå›ç­”æ–¹å¼ã€
* value: é€™å€‹è¨Šæ¯çš„å¯¦éš›å…§å®¹
  * Value æ˜¯ token çš„èªæ„å…§å®¹ï¼Œæ˜¯ç•¶é€™å€‹ token è¢«é—œæ³¨æ™‚ï¼Œå¯¦éš›æä¾›å‡ºä¾†çš„è³‡è¨Š
  * Attention åˆ†æ•¸å°±æ˜¯æ±ºå®šï¼šè¦æŠŠé€™å€‹ Value åŠ å¤šå°‘æ¬Šé‡æ”¾é€² output ä¸­
* çµè«–:
  * ã€ŒAttention æ˜¯ä¸€ç¨®åŸºæ–¼ Qâ€“K ç›¸ä¼¼åº¦çš„åŠ æ¬ŠæŸ¥è©¢ï¼Œå¾æ‰€æœ‰ token çš„ Value ä¸­æ“·å–è³‡è¨Šï¼Œçµ„åˆå‡º contextual è¡¨ç¤ºã€‚ã€

2.1.2. Low-Rank Key-Value Joint Compression
* The core of MLA is the low-rank joint compression for keys and values to reduce KV cache:
![image](https://github.com/user-attachments/assets/19d3de7f-50ba-4d63-ad23-1e652a221c28)

* $c_t^{KV} \in \mathbb{R}^{d_c}$ is the compressed latent vector for keys and values;
* $d_c(\ll d_h n_h)$ denotes the KV compression dimension;
* $W^{DKV} \in \mathbb{R}^{d_c \times d}$ is the down-projection matrix;
* $W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ are the up-projection matrices for keys and values, respectively.
* During inference,
  * MLA only needs to cache $c_t^{KV}$, so its KV cache has only $d_c l$ elements, where ğ‘™ denotes the number of layers.
  * $W^{UK}$ can be absorbed into $W^Q$
  * $W^{UV}$ can be absorbed into $W^O$
    * => we even do not need to compute keys and values out for attention.
* Figure 3 intuitively illustrates how the KV joint compression in MLA reduces the KV cache
![image](https://github.com/user-attachments/assets/8868b2d6-7093-4bcf-a353-211a75412d8d)
Figure 3 | Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through
jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV
cache during inference.
* in order to reduce the activation memory during training, we also perform low-rank compression for the queries, even if it cannot reduce the KV cache:
![image](https://github.com/user-attachments/assets/7b0c7dc8-9437-4aa3-8963-680dd2e41905)
  * $c_t^{Q} \in \mathbb{R}^{d_c'}$ is the compressed latent vector for queries;
  * $d_c'(\ll d_h n_h)$ denotes the query compression dimension;\
  * $W^{DQ} \in \mathbb{R}^{d_c' \times d}$ is the down-projection matrices
  * $W^{UQ} \in \mathbb{R}^{d_h n_h \times d_c'}$ is the upprojection matrices

2.1.3. Decoupled Rotary Position Embedding

#### Rotary Position Embedding (RoPE) è§£é‡‹ by DeepSeek V3: https://chat.deepseek.com/a/chat/s/262576df-35bd-4615-8ae0-85b3ee45688a
Rotary Position Embedding (RoPE) æ˜¯ä¸€ç¨®ç”¨æ–¼ Transformer æ¨¡å‹çš„ä½ç½®ç·¨ç¢¼æ–¹æ³•ï¼Œæ—¨åœ¨æ›´å¥½åœ°æ•æ‰åºåˆ—ä¸­å…ƒç´ çš„ä½ç½®ä¿¡æ¯ã€‚ä»¥ä¸‹æ˜¯å…¶é—œéµé»ï¼š

- èƒŒæ™¯
  - Transformer æ¨¡å‹ä¾è³´ä½ç½®ç·¨ç¢¼ä¾†ç†è§£åºåˆ—ä¸­å…ƒç´ çš„é †åºã€‚
  - å‚³çµ±æ–¹æ³•å¦‚æ­£å¼¦å’Œé¤˜å¼¦å‡½æ•¸ç·¨ç¢¼ä½ç½®ä¿¡æ¯ï¼Œä½†å¯èƒ½ç„¡æ³•å……åˆ†æ•æ‰ç›¸å°ä½ç½®é—œä¿‚ã€‚
- RoPE çš„æ ¸å¿ƒæ€æƒ³
  - é€šéæ—‹è½‰çŸ©é™£å°‡ä½ç½®ä¿¡æ¯åµŒå…¥åˆ°è©å‘é‡ä¸­ã€‚
  - æ¯å€‹ä½ç½®å°æ‡‰ä¸€å€‹æ—‹è½‰è§’åº¦ï¼Œè©å‘é‡æ ¹æ“šå…¶ä½ç½®é€²è¡Œæ—‹è½‰ï¼Œå¾è€Œç·¨ç¢¼ä½ç½®ä¿¡æ¯ã€‚
- å¯¦ç¾æ–¹å¼
  - å°è©å‘é‡æ‡‰ç”¨æ—‹è½‰çŸ©é™£ï¼Œæ—‹è½‰è§’åº¦èˆ‡ä½ç½®ç›¸é—œã€‚
  - æ—‹è½‰å¾Œçš„å‘é‡ä¿ç•™äº†ä½ç½®ä¿¡æ¯ï¼ŒåŒæ™‚ä¿æŒäº†å‘é‡çš„å¹¾ä½•ç‰¹æ€§ã€‚
- å„ªé»
  - **ç›¸å°ä½ç½®ç·¨ç¢¼**ï¼šèƒ½æ›´å¥½åœ°æ•æ‰å…ƒç´ é–“çš„ç›¸å°ä½ç½®é—œä¿‚ã€‚
  - **å¹¾ä½•ç‰¹æ€§ä¿æŒ**ï¼šæ—‹è½‰æ“ä½œä¿æŒäº†å‘é‡çš„å¹¾ä½•çµæ§‹ï¼Œæœ‰åŠ©æ–¼æ¨¡å‹ç†è§£ä½ç½®é—œä¿‚ã€‚
  - **éˆæ´»æ€§**ï¼šé©ç”¨æ–¼ä¸åŒé•·åº¦å’Œçµæ§‹çš„åºåˆ—ã€‚
- æ‡‰ç”¨
  - ä¸»è¦ç”¨æ–¼è‡ªç„¶èªè¨€è™•ç†ä»»å‹™ï¼Œå¦‚æ©Ÿå™¨ç¿»è­¯ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚
  - ä¹Ÿå¯ç”¨æ–¼å…¶ä»–éœ€è¦è™•ç†åºåˆ—æ•¸æ“šçš„é ˜åŸŸï¼Œå¦‚æ™‚é–“åºåˆ—åˆ†æã€‚
- ç¸½çµ: RoPE é€šéæ—‹è½‰çŸ©é™£å°‡ä½ç½®ä¿¡æ¯åµŒå…¥è©å‘é‡ï¼Œæå‡äº†æ¨¡å‹å°åºåˆ—ä½ç½®é—œä¿‚çš„æ•æ‰èƒ½åŠ›ã€‚

![image](https://github.com/user-attachments/assets/233751a2-ec84-45b6-9771-623471943c66)

2.1.4. Comparison of Key-Value Cache
![image](https://github.com/user-attachments/assets/ec91ff04-ffc1-4088-a6f9-1746893a73ef)
* $n_h$: the number of attention heads
* $d_h$: the dimension per attention head
* $ğ‘™$: the number of layers
* $n_g$: the number of groups in GQA
* $d_c$: the KV compression dimension in MLA
* $d_h^R$: the per-head dimension of the decoupled queries and key in MLA
* The amount of KV cache is measured by the number of elements, regardless of the storage precision
  * For DeepSeek-V2, $d_c$ is set to $4 d_h$, $d_h^R$ is set to $\frac{d_h}{2}$

2.2. DeepSeekMoE: Training Strong Models at Economical Costs

2.2.1. Basic Architecture
* For FFNs, we employ the DeepSeekMoE architecture (Dai et al., 2024)
  * two key ideas:
    * segmenting experts into finer granularity for higher expert specialization and more accurate knowledge acquisition
    * isolating some shared experts for mitigating knowledge redundancy among routed experts
* Let $u_t$ be the FFN input of the t-th token, we compute the FFN output $h_t^{\prime}$ as follows:
  
  $h_t^{\prime} = u_t + \displaystyle\sum_{i=1}^{N_s} FFN_i^{(s)}(u_t) + \displaystyle\sum_{i=1}^{N_r} g_{i,t} FFN_i^{(r)}(u_t),$ (20)

  ![image](https://github.com/user-attachments/assets/3c959bc7-0c15-46df-9578-4fe4201dbc4e)

  $s_{i,t} = Softmax_i({u_t}^Te_i),$ (22)


## high-flyer
* https://www.high-flyer.cn/blog/llama2-1/
* https://github.com/deepseek-ai/DeepSeek-VL2


