# Contents
  * Kubernetes (k8s)
  * ASAP - real2sim2real
  * DeepSeek
    * v2
  * high-flyer

## Kubernetes (k8s)
* Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation.
* Virtualized deployment era:
  * Each VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware.
* Container deployment era:
  * Containers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications.
    Therefore, containers are considered lightweight.
  * ä¸ä¼ ç»Ÿçš„è™šæ‹Ÿæœºä¸åŒï¼Œå®¹å™¨å…±äº«ä¸»æœºæ“ä½œç³»ç»Ÿçš„å†…æ ¸ï¼Œå› æ­¤æ›´åŠ é«˜æ•ˆå’Œä¾¿æºã€‚ 

## ASAP - real2sim2real
https://hao.cnyes.com/post/133793

## DeepSeek
### DeepSeek-VL2 - 2024 Jun 19
* https://github.com/deepseek-ai/DeepSeek-VL2

### DeepSeek-V2
* https://github.com/deepseek-ai/DeepSeek-V2
* a strong Mixture-of-Experts (MoE) language model
* 236B total parameters, 21B are activated for each token,  context length of 128K tokens
* innovative architectures
  * Multi-head Latent Attention (MLA)
    * MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector
  * DeepSeekMoE
    * enables training strong models at an economical cost through sparse computation
  * v.s. DeepSeek 67B:
    * significantly stronger performance
    * saves 42.5% of training costs
    * reduces the KV cache by 93.3%
    * boosts the maximum generation throughput to 5.76 times
  * pretrain: on a high-quality and multi-source corpus consisting of 8.1T tokens
  * Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)
![image](https://github.com/user-attachments/assets/bde50e68-af54-4627-8dd2-239be143dbe6)

1. Introduction
* we introduce MLA, an attention mechanism equipped with low-rank key-value joint compression.
  Empirically, MLA achieves superior performance compared with MHA, significantly reduces the KV cache during inference, thus boosting the inference efficiency.
* For Feed-Forward Networks (FFNs), we follow the DeepSeekMoE architecture (Dai et al., 2024), which adopts
  fine-grained expert segmentation and shared expert isolation for higher potential in expert specialization 
* We construct a high-quality and multi-source pre-training corpus consisting of 8.1T tokens.
* DeepSeek-V2 Chat (SFT): we collect 1.5M conversational sessions, which encompass various domains such as math, code, 
  writing, reasoning, safety, and more, to perform Supervised Fine-Tuning (SFT)  
* DeepSeek-V2 Chat (RL): Finally, we employ Group Relative Policy Optimization (GRPO) to further align the model with
  human preference
![image](https://github.com/user-attachments/assets/44477fe4-c33f-4cce-9cc4-40c9f1257ed8)
* We evaluate DeepSeek-V2 on a wide range of benchmarks in English and Chinese
  * even with only 21B activated parameters, V2 is the strongest open-source MoE language model
  * compared with DeepSeek 67B, V2 saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the
    maximum generation throughput to 5.76 times
  * V2 Chat (RL) achieves
    - 38.9 length-controlled win rate on AlpacaEval 2.0 (Dubois et al., 2024),
    - 8.97 overall score on MT-Bench (Zheng et al., 2023), and
    - 7.91 overall score on AlignBench (Liu et al., 2023).
  * DeepSeek-V2-Lite
    - 15.7B parameters
    - 2.4B are activated for each token.

2. Architecture
  * For attention, we design MLA, which utilizes low-rank key-value joint compression to
    eliminate the bottleneck of inference-time key-value cache, thus supporting efficient inference.
  * For Feed-Forward Networks (FFNs), we adopt the DeepSeekMoE architecture (Dai et al., 2024), a
    high-performance MoE architecture that enables training strong models at an economical cost.

2.1. Multi-Head Latent Attention: Boosting Inference Efficiency

2.1.1. Preliminaries: Standard Multi-Head Attention

#### Multi-Head Attention (MHA) è§£é‡‹ by DeepSeek V3:
* https://chat.deepseek.com/a/chat/s/e579d7f5-60ad-4fa2-87c3-88b37221b0a2

Multi-Head Attention (MHA) æ˜¯ Transformer æ¨¡å‹ä¸­çš„æ ¸å¿ƒæ©Ÿåˆ¶ï¼Œç”¨æ–¼æ•æ‰è¼¸å…¥åºåˆ—ä¸­ä¸åŒä½ç½®çš„é—œä¿‚ã€‚ä»¥ä¸‹æ˜¯å…¶é—œéµé»ï¼š
* Self-Attention åŸºç¤
  * é€šéè¨ˆç®—è¼¸å…¥åºåˆ—ä¸­æ¯å€‹å…ƒç´ èˆ‡å…¶ä»–å…ƒç´ çš„ç›¸é—œæ€§ï¼Œç”ŸæˆåŠ æ¬Šè¡¨ç¤ºã€‚
  * è¼¸å…¥åºåˆ— $$X = (x_1, x_2, \dots, x_n)$$\
    é€šéç·šæ€§è®Šæ›å¾—åˆ°æŸ¥è©¢ï¼ˆQueryï¼‰ã€éµï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰å‘é‡ï¼š\
    $$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$
  * æ³¨æ„åŠ›å¾—åˆ†é€šéé»ç©è¨ˆç®—ï¼š\
    $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$\
    å…¶ä¸­ $$\( d_k \)$$ æ˜¯éµå‘é‡çš„ç¶­åº¦ã€‚

* Multi-Head Attention
  - é€šéä¸¦è¡Œè¨ˆç®—å¤šå€‹ Self-Attention é ­ï¼Œæ•æ‰ä¸åŒçš„ç‰¹å¾µè¡¨ç¤ºã€‚
  - æ¯å€‹é ­ç¨ç«‹è¨ˆç®—æ³¨æ„åŠ›ï¼š\
    $head_i = \text{Attention}(QW_{Q_i}, KW_{K_i}, VW_{V_i})$

  - æ‰€æœ‰é ­çš„è¼¸å‡ºæ‹¼æ¥å¾Œé€šéç·šæ€§è®Šæ›ï¼š\
    $\text{MHA}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W_O$\
    $å…¶ä¸­ \ h \ æ˜¯é ­æ•¸ï¼Œ\ W_O \ æ˜¯è¼¸å‡ºæ¬Šé‡çŸ©é™£ã€‚$

* å„ªé»
  - **ä¸¦è¡Œè¨ˆç®—**ï¼šå¤šå€‹é ­å¯ä»¥åŒæ™‚è™•ç†ä¸åŒç‰¹å¾µã€‚
  - **å¤šæ¨£åŒ–ç‰¹å¾µæ•æ‰**ï¼šæ¯å€‹é ­é—œæ³¨ä¸åŒçš„å­ç©ºé–“ï¼Œå¢å¼·æ¨¡å‹è¡¨é”èƒ½åŠ›ã€‚

* æ‡‰ç”¨
  - **Transformer**ï¼šMHA æ˜¯å…¶ç·¨ç¢¼å™¨å’Œè§£ç¢¼å™¨çš„æ ¸å¿ƒã€‚
  - **BERTã€GPT**ï¼šé€™äº›é è¨“ç·´æ¨¡å‹ä¹Ÿä¾è³´ MHA é€²è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚

* ç¸½çµ
  - **Multi-Head Attention** é€šéä¸¦è¡Œè¨ˆç®—å¤šå€‹ Self-Attention é ­ï¼Œå¢å¼·äº†æ¨¡å‹æ•æ‰å¤šæ¨£åŒ–ç‰¹å¾µçš„èƒ½åŠ›ï¼Œæ˜¯ Transformer æ¨¡å‹æˆåŠŸçš„é—œéµã€‚

#### Multi-Head Attention (MHA) è§£é‡‹ by This paper 2.1.1:

$\text{Let } d \text{ be the embedding dimension, }$\
$n_h \text{ be the number of attention heads, }$\
$d_h \text{ be the dimension per head, and }$\
$h_t \in \mathbb{R}^d \text{ be the attention input of the ğ‘¡-th token at an attention layer.}$\
Standard MHA first produces $q_t, k_t, v_t \in \mathbb{R}^{d_h n_h}$ through three matrices $W^Q, W^K, W^V \in \mathbb{R}^{d_h n_h \times d}$
![image](https://github.com/user-attachments/assets/812a3f6e-4157-41e6-a204-f28eec731fdf)

Then, $q_t, k_t, v_t$ will be sliced into $n_h$ heads for the multi-head attention computation:
![image](https://github.com/user-attachments/assets/37c9eb32-0668-4a36-9d21-b9e9730ba823)

* where $q_{t,i}, k_{t,i}, v_{t,i} \in \mathbb{R}^{d_h}$ denote the query, key, and value of the ğ‘–-th attention head, respectively;
* $W^o \in \mathbb{R}^{d \times d_h n_h}$ denotes the output projection matrix.
* During inference, all keys and values need to be cached to accelerate inference, so MHA needs to cache $2n_h d_h l$ elements for each token. 
* In model deployment, this heavy KV cache is a large bottleneck that limits the maximum batch size and sequence length.

æˆ‘çš„ç†è§£:
æ­¤è¡¨ä¾†è‡ª chatgpt 4o:
* https://chatgpt.com/share/682dc278-486c-800c-b0e5-6dd121f2dfdb
* 
| æ¦‚å¿µ               | æ„ç¾©               | ç›´è§€æ¯”å–»        |
| ---------------- | ---------------- | ----------- |
| Query            | æˆ‘è¦é—œæ³¨ä»€éº¼ï¼Ÿ          | ç™¼å•è€…         |
| Key              | æˆ‘èƒ½æä¾›ä»€éº¼è³‡è¨Šï¼Ÿ        | å€™é¸ç·šç´¢        |
| Value            | æˆ‘çš„å¯¦éš›å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ       | æœ€å¾Œå–ç”¨çš„ç­”æ¡ˆ     |
| Attention(Q,K,V) | æ‰¾å‡ºæœ€ relevant çš„å…§å®¹ | å•å°å•é¡Œã€æ‰¾åˆ°å°çš„è³‡æ–™ |

* æ¯å€‹ head é—œæ³¨ä¸åŒçš„è§’åº¦ (æå‡ºä¸åŒçš„å•é¡Œ)
  * æ¯å€‹ head æœ‰è‡ªå·±çš„ä¸€å¥—æŠ•å½±çŸ©é™£ï¼ˆQ/K/Vï¼‰ï¼Œæ‰€ä»¥å®ƒå¯ä»¥å¾ä¸åŒã€Œèªæ„å­ç©ºé–“ã€å»åˆ†æè³‡æ–™ã€‚
  * æœ‰çš„ head å°ˆæ³¨èªæ³•ï¼Œæœ‰çš„å°ˆæ³¨èªæ„ï¼Œæœ‰çš„é—œæ³¨ç›¸å°ä½ç½®ã€‚
* query: å°ä¸€å€‹ token æå‡ºæŸå€‹ head çš„å•é¡Œ
  * Query æ˜¯é€™å€‹ token åœ¨è©² head è£¡ç™¼å‡ºçš„ä¸€å€‹æŸ¥è©¢å‘é‡
  * è©¦åœ–å•ï¼šã€Œæˆ‘è©²é—œæ³¨èª°ï¼Ÿèª°èˆ‡æˆ‘æœ‰èªæ„ä¸Šçš„é€£çµï¼Ÿã€
* key: é€™å€‹ token å°æŸå€‹ head èƒ½æä¾›ä»€éº¼è¨Šæ¯
  * Key æ˜¯å…¶ä»– token å°æ‡‰çš„ã€Œè‡ªæˆ‘æè¿°ã€ï¼šæˆ‘æ˜¯èª°ï¼Ÿæˆ‘åœ¨é€™å€‹èªå¢ƒä¸­æœ‰ä»€éº¼è§’è‰²ï¼Ÿ
  * æ¯å€‹ token å°æ¯å€‹ head æœƒæœ‰ä¸åŒçš„ Keyï¼Œå°æ‡‰ä¸åŒçš„ã€Œå›ç­”æ–¹å¼ã€
* value: é€™å€‹è¨Šæ¯çš„å¯¦éš›å…§å®¹
  * Value æ˜¯ token çš„èªæ„å…§å®¹ï¼Œæ˜¯ç•¶é€™å€‹ token è¢«é—œæ³¨æ™‚ï¼Œå¯¦éš›æä¾›å‡ºä¾†çš„è³‡è¨Š
  * Attention åˆ†æ•¸å°±æ˜¯æ±ºå®šï¼šè¦æŠŠé€™å€‹ Value åŠ å¤šå°‘æ¬Šé‡æ”¾é€² output ä¸­
* çµè«–:
  * ã€ŒAttention æ˜¯ä¸€ç¨®åŸºæ–¼ Qâ€“K ç›¸ä¼¼åº¦çš„åŠ æ¬ŠæŸ¥è©¢ï¼Œå¾æ‰€æœ‰ token çš„ Value ä¸­æ“·å–è³‡è¨Šï¼Œçµ„åˆå‡º contextual è¡¨ç¤ºã€‚ã€

2.1.2. Low-Rank Key-Value Joint Compression
* The core of MLA is the low-rank joint compression for keys and values to reduce KV cache:
![image](https://github.com/user-attachments/assets/19d3de7f-50ba-4d63-ad23-1e652a221c28)

* $c_t^{KV} \in \mathbb{R}^{d_c}$ is the compressed latent vector for keys and values;
* $d_c(\ll d_h n_h)$ denotes the KV compression dimension;
* $W^{DKV} \in \mathbb{R}^{d_c \times d}$ is the down-projection matrix;
* $W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ are the up-projection matrices for keys and values, respectively.
* During inference,
  * MLA only needs to cache $c_t^{KV}$, so its KV cache has only $d_c l$ elements, where ğ‘™ denotes the number of layers.
  * $W^{UK}$ can be absorbed into $W^Q$
  * $W^{UV}$ can be absorbed into $W^O$
    * => we even do not need to compute keys and values out for attention.
* Figure 3 intuitively illustrates how the KV joint compression in MLA reduces the KV cache
![image](https://github.com/user-attachments/assets/8868b2d6-7093-4bcf-a353-211a75412d8d)
Figure 3 | Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through
jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV
cache during inference.
* in order to reduce the activation memory during training, we also perform low-rank compression for the queries, even if it cannot reduce the KV cache:
![image](https://github.com/user-attachments/assets/7b0c7dc8-9437-4aa3-8963-680dd2e41905)
  * $c_t^{Q} \in \mathbb{R}^{d_c'}$ is the compressed latent vector for queries;
  * $d_c'(\ll d_h n_h)$ denotes the query compression dimension;\
  * $W^{DQ} \in \mathbb{R}^{d_c' \times d}$ is the down-projection matrices
  * $W^{UQ} \in \mathbb{R}^{d_h n_h \times d_c'}$ is the upprojection matrices

2.1.3. Decoupled Rotary Position Embedding

#### Rotary Position Embedding (RoPE) è§£é‡‹ by DeepSeek V3: https://chat.deepseek.com/a/chat/s/262576df-35bd-4615-8ae0-85b3ee45688a
Rotary Position Embedding (RoPE) æ˜¯ä¸€ç¨®ç”¨æ–¼ Transformer æ¨¡å‹çš„ä½ç½®ç·¨ç¢¼æ–¹æ³•ï¼Œæ—¨åœ¨æ›´å¥½åœ°æ•æ‰åºåˆ—ä¸­å…ƒç´ çš„ä½ç½®ä¿¡æ¯ã€‚ä»¥ä¸‹æ˜¯å…¶é—œéµé»ï¼š

- èƒŒæ™¯
  - Transformer æ¨¡å‹ä¾è³´ä½ç½®ç·¨ç¢¼ä¾†ç†è§£åºåˆ—ä¸­å…ƒç´ çš„é †åºã€‚
  - å‚³çµ±æ–¹æ³•å¦‚æ­£å¼¦å’Œé¤˜å¼¦å‡½æ•¸ç·¨ç¢¼ä½ç½®ä¿¡æ¯ï¼Œä½†å¯èƒ½ç„¡æ³•å……åˆ†æ•æ‰ç›¸å°ä½ç½®é—œä¿‚ã€‚
- RoPE çš„æ ¸å¿ƒæ€æƒ³
  - é€šéæ—‹è½‰çŸ©é™£å°‡ä½ç½®ä¿¡æ¯åµŒå…¥åˆ°è©å‘é‡ä¸­ã€‚
  - æ¯å€‹ä½ç½®å°æ‡‰ä¸€å€‹æ—‹è½‰è§’åº¦ï¼Œè©å‘é‡æ ¹æ“šå…¶ä½ç½®é€²è¡Œæ—‹è½‰ï¼Œå¾è€Œç·¨ç¢¼ä½ç½®ä¿¡æ¯ã€‚
- å¯¦ç¾æ–¹å¼
  - å°è©å‘é‡æ‡‰ç”¨æ—‹è½‰çŸ©é™£ï¼Œæ—‹è½‰è§’åº¦èˆ‡ä½ç½®ç›¸é—œã€‚
  - æ—‹è½‰å¾Œçš„å‘é‡ä¿ç•™äº†ä½ç½®ä¿¡æ¯ï¼ŒåŒæ™‚ä¿æŒäº†å‘é‡çš„å¹¾ä½•ç‰¹æ€§ã€‚
- å„ªé»
  - **ç›¸å°ä½ç½®ç·¨ç¢¼**ï¼šèƒ½æ›´å¥½åœ°æ•æ‰å…ƒç´ é–“çš„ç›¸å°ä½ç½®é—œä¿‚ã€‚
  - **å¹¾ä½•ç‰¹æ€§ä¿æŒ**ï¼šæ—‹è½‰æ“ä½œä¿æŒäº†å‘é‡çš„å¹¾ä½•çµæ§‹ï¼Œæœ‰åŠ©æ–¼æ¨¡å‹ç†è§£ä½ç½®é—œä¿‚ã€‚
  - **éˆæ´»æ€§**ï¼šé©ç”¨æ–¼ä¸åŒé•·åº¦å’Œçµæ§‹çš„åºåˆ—ã€‚
- æ‡‰ç”¨
  - ä¸»è¦ç”¨æ–¼è‡ªç„¶èªè¨€è™•ç†ä»»å‹™ï¼Œå¦‚æ©Ÿå™¨ç¿»è­¯ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚
  - ä¹Ÿå¯ç”¨æ–¼å…¶ä»–éœ€è¦è™•ç†åºåˆ—æ•¸æ“šçš„é ˜åŸŸï¼Œå¦‚æ™‚é–“åºåˆ—åˆ†æã€‚
- ç¸½çµ: RoPE é€šéæ—‹è½‰çŸ©é™£å°‡ä½ç½®ä¿¡æ¯åµŒå…¥è©å‘é‡ï¼Œæå‡äº†æ¨¡å‹å°åºåˆ—ä½ç½®é—œä¿‚çš„æ•æ‰èƒ½åŠ›ã€‚

![image](https://github.com/user-attachments/assets/233751a2-ec84-45b6-9771-623471943c66)

2.1.4. Comparison of Key-Value Cache
![image](https://github.com/user-attachments/assets/ec91ff04-ffc1-4088-a6f9-1746893a73ef)
* $n_h$: the number of attention heads
* $d_h$: the dimension per attention head
* $ğ‘™$: the number of layers
* $n_g$: the number of groups in GQA
* $d_c$: the KV compression dimension in MLA
* $d_h^R$: the per-head dimension of the decoupled queries and key in MLA
* The amount of KV cache is measured by the number of elements, regardless of the storage precision
  * For DeepSeek-V2, $d_c$ is set to $4 d_h$, $d_h^R$ is set to $\frac{d_h}{2}$

2.2. DeepSeekMoE: Training Strong Models at Economical Costs

2.2.1. Basic Architecture
* For FFNs, we employ the DeepSeekMoE architecture (Dai et al., 2024)
  * two key ideas:
    * segmenting experts into finer granularity for higher expert specialization and more accurate knowledge acquisition
    * isolating some shared experts for mitigating knowledge redundancy among routed experts
* Let $u_t$ be the FFN input of the t-th token, we compute the FFN output $h_t^{\prime}$ as follows:
  
  $h_t^{\prime} = u_t + \displaystyle\sum_{i=1}^{N_s} FFN_i^{(s)}(u_t) + \displaystyle\sum_{i=1}^{N_r} g_{i,t} FFN_i^{(r)}(u_t),$ (20)

  ![image](https://github.com/user-attachments/assets/3c959bc7-0c15-46df-9578-4fe4201dbc4e)

  $s_{i,t} = Softmax_i({u_t}^Te_i),$ (22)

  * $N_s$: the numbers of shared experts
  * $N_r$: the numbers of routed experts
  * $FFN_i^{(s)}(\bullet)$: the ğ‘–-th shared expert
  * $FFN_i^{(r)}(\bullet)$: the ğ‘–-th routed expert
  * $K_r$: the number of activated routed experts
  * $g_{i,t}$: the gate value for the ğ‘–-th expert
  * $s_{i,t}$: the token-to-expert affinity
  * $e_i$: the centroid of the ğ‘–-th routed expert in this layer
  * $Topk(\bullet,K)$: the set comprising ğ¾ highest scores among the affinity scores calculated for the ğ‘¡-th token and all routed experts.

#### FFN è§£é‡‹ by ChatGPT 4o
* https://chatgpt.com/share/682dc278-486c-800c-b0e5-6dd121f2dfdb
* åœ¨ Transformer ä¸­ï¼ŒFFN æ˜¯ä½œç”¨åœ¨æ¯å€‹ token çš„ç¨ç«‹å…©å±¤å‰é¥‹ç¥ç¶“ç¶²è·¯ã€‚
* é—œéµç‰¹æ€§ï¼š
  * å®ƒä¸è€ƒæ…®ä¸Šä¸‹æ–‡æˆ–å…¶ä»– tokenï¼ˆä¸åƒ attentionï¼‰
  * å®ƒå°åºåˆ—ä¸­æ¯å€‹ token çš„å‘é‡ï¼Œé€å€‹ã€ç¨ç«‹åœ°è™•ç†
  * è² è²¬æä¾›éç·šæ€§è½‰æ›èˆ‡è³‡è¨Šæ“´æ•£
* æ•¸å­¸å®šç¾©
  * å‡è¨­è¼¸å…¥å‘é‡ç‚º $x \in \mathbb{R}^d$, FFN çš„é‹ç®—å¦‚ä¸‹ï¼š
    
    $FFN(x) = W_2 \bullet \sigma(W_1x + b_1) + b_2$

    * $W_1 \in \mathbb{R}^{d_{ff} \times d}$, ç¬¬ä¸€å±¤ï¼šå‡ç¶­, æ‹“å±•èªç¾©ç©ºé–“
      * é€™æ˜¯ä¸€å€‹ ç·šæ€§æŠ•å½±ï¼Œè®“æ¨¡å‹æœ‰æ›´å¤šç©ºé–“åšè¤‡é›œçš„ç‰¹å¾µè½‰æ›ã€‚
      * è³¦äºˆ token æ›´é«˜ç¶­çš„èªç¾©è‡ªç”±åº¦: semantic expansionï¼Œä¾‹å¦‚ï¼š
        * æƒ…ç·’æˆåˆ†
        * èªæ³•é¡å‹
        * éš±å–»/æ¯”å–»æ€§
      * é€™ä¸€å±¤ä¸æ˜¯ç°¡å–®åœ°æ‹‰å¤§ç¶­åº¦ï¼Œè€Œæ˜¯è®“æ¨¡å‹èƒ½å­¸åˆ°æ›´è±å¯Œçš„èªæ„ç‰¹å¾µçµ„åˆã€‚
    * $\sigma$ï¼šéç·šæ€§å•Ÿå‹•å‡½æ•¸ï¼ˆé€šå¸¸æ˜¯ ReLU æˆ– GELUï¼‰
      * è®“ç¥ç¶“ç¶²è·¯ä¸åªæ˜¯ç·šæ€§è®Šæ›çš„å †ç–Š
      * æ¯å€‹ç¥ç¶“å…ƒå¯ä»¥ã€Œé¸æ“‡æ€§å•Ÿå‹•ã€ï¼ˆå•Ÿå‹•æœ‰æ„ç¾©çš„ç‰¹å¾µï¼ŒæŠ‘åˆ¶ç„¡ç”¨çš„ï¼‰
      * æ¨¡æ“¬å¤§è…¦ä¸­ä¸åŒç¥ç¶“å…ƒè¢«ç‰¹å®šè¼¸å…¥æ¨¡å¼æ¿€æ´»
    * $W_2 \in \mathbb{R}^{d \times d_{ff}}$, ç¬¬äºŒå±¤ï¼šé™ç¶­, èåˆè³‡è¨Š, è¼¸å‡ºå°é½Š
      * é€™æ˜¯ç¬¬äºŒå€‹ç·šæ€§è®Šæ›ï¼Œè®“ FFN çš„è¼¸å‡ºè·ŸåŸä¾†çš„ token embedding ç¶­åº¦ä¸€è‡´ï¼ˆä¾‹å¦‚ 768 ç¶­ï¼‰ï¼Œä»¥ä¾¿ residual connection å’Œå¾ŒçºŒè™•ç†ã€‚
      * å°‡å¤šå€‹èªæ„å­ç‰¹å¾µã€Œå£“ç¸®ã€å› token åŸå§‹å‘é‡ç©ºé–“
      * semantic synthesis
      * é¡ä¼¼æ–¼ï¼šæˆ‘å€‘è§€å¯Ÿä¸€å€‹è©å¾å¤šè§’åº¦åˆ†æå¾Œï¼ŒæŠŠçµè«–æ•´ç†å›ä¸»å‘é‡ä¸­
    * $b_1, b_2$ï¼šåå·®é …ï¼ˆbiasï¼‰
* ç‚ºä»€éº¼è¦æœ‰å…©å±¤ç·šæ€§è½‰æ› + éç·šæ€§ï¼Ÿ
  * Transformer çš„æ³¨æ„åŠ›æ©Ÿåˆ¶ä¸»è¦è² è²¬ï¼šæ•æ‰ä¸åŒ token ä¹‹é–“çš„é—œä¿‚ï¼ˆä¸Šä¸‹æ–‡ç›¸é—œæ€§ï¼‰
  * ä½†é‚„ä¸å¤ ï¼Œå› ç‚ºèªè¨€çš„è±å¯Œæ€§ä¸åªæ˜¯é—œä¿‚ï¼Œé‚„åŒ…æ‹¬ï¼š
    * ä¸€å€‹ token è‡ªèº«çš„èªç¾©å¦‚ä½•è½‰æ›ï¼Ÿ
    * å¦‚ä½•æ ¹æ“šä¸Šä¸‹æ–‡é‡æ§‹ token å‘é‡ï¼Ÿ
    * å¦‚ä½•çµ„åˆå­è©ã€å½¢æ…‹ã€æƒ…ç·’ç­‰å¤šå±¤æ¬¡ç‰¹å¾µï¼Ÿ
  * é€™äº›å°±äº¤çµ¦ Feed-Forward Networkï¼ˆFFNï¼‰ ä¾†åšï¼
* å¸¸è¦‹è¨­å®š
  | æ¨¡å‹          | $d$ï¼ˆè¼¸å…¥ç¶­åº¦ï¼‰ | $d_{\text{ff}}$ï¼ˆä¸­é–“å±¤ï¼‰ |
  | ----------- | --------- | -------------------- |
  | BERT Base   | 768       | 3072ï¼ˆå³ 4Ã—768ï¼‰        |
  | GPT-2 Small | 768       | 3072                 |
  | GPT-3       | 12288     | 49152                |
  | LLaMA 2-7B  | 4096      | 11008ï¼ˆç´„ 2.7Ã—ï¼‰        |

  é€šå¸¸ FFN çš„ä¸­é–“ç¶­åº¦è¨­ç‚ºè¼¸å…¥ç¶­åº¦çš„ 4 å€ï¼ˆæˆ– 2.7ï½4 å€ï¼‰ï¼Œé€™æ˜¯ç‚ºäº†è®“æ¨¡å‹èƒ½åšæ›´è±å¯Œçš„è®Šæ›ã€‚
* åœ¨ Transformer è£¡çš„ä½ç½®
  ```
  x â”€â”€â–¶ Self-Attention â”€â”€â–¶ Add + LayerNorm â”€â”€â–¶ FFN â”€â”€â–¶ Add + LayerNorm
  ```
  FFN æ¥åœ¨æ³¨æ„åŠ›å±¤å¾Œé¢ï¼Œåšã€Œä½ç½®å…§éƒ¨ã€çš„ç‰¹å¾µè½‰æ›ã€‚
* ç‚ºä»€éº¼è¦æœ‰ FFNï¼Ÿ
  | ç›®çš„             | èªªæ˜                               |
  | -------------- | -------------------------------- |
  | æä¾›éç·šæ€§è½‰æ›        | å¢å¼·æ¨¡å‹çš„è¡¨é”èƒ½åŠ›                        |
  | å°æ¯å€‹ token ç¨ç«‹è™•ç† | æ³¨æ„åŠ›æ•æ‰ token é–“é—œä¿‚ï¼ŒFFN å°ˆæ³¨åœ¨ token æœ¬èº« |
  | è±å¯Œæ¯å±¤çš„åŠŸèƒ½        | æ¨¡å‹ä¸åªèƒ½æ•æ‰é—œä¿‚ï¼Œé‚„èƒ½é€²è¡Œç‰¹å¾µå¼·åŒ–æˆ–å£“ç¸®            |
* é€²éšè®Šé«”
  | è®Šé«”               | èªªæ˜                                                       |
  | ---------------- | -------------------------------------------------------- |
  | **SwiGLU**       | å°‡å•Ÿå‹•å‡½æ•¸æ›¿æ›ç‚º gated variantï¼Œæ›´ç©©å®š                               |
  | **MoE-FFN**      | ä½¿ç”¨ç¨€ç– expert åˆ†æ”¯ï¼ˆMixture-of-Expertsï¼‰ï¼Œä¾‹å¦‚ Switch Transformer |
  | **Low-Rank FFN** | é™ä½åƒæ•¸é‡ï¼Œæé«˜æ¨ç†æ•ˆç‡ï¼ˆç”¨æ–¼è¼•é‡åŒ–æ¨¡å‹ï¼‰                                    |
* å°çµ
  | é …ç›®      | èªªæ˜                                   |
  | ------- | ------------------------------------ |
  | FFN åŠŸèƒ½  | å°æ¯å€‹ token å‘é‡åšéç·šæ€§è½‰æ›èˆ‡å‡é™ç¶­               |
  | æ˜¯å¦çœ‹ä¸Šä¸‹æ–‡ï¼Ÿ | âŒ å¦ï¼Œtoken ç¨ç«‹è™•ç†                       |
  | æ­é…ä»€éº¼ä½¿ç”¨ï¼Ÿ | Attention + Residual + LayerNorm     |
  | è¨ˆç®—é–‹éŠ·    | ç›¸å° attention è¼ƒå°ï¼Œä½†åœ¨å¤§å‹æ¨¡å‹ä¸­ä»å é¡¯è‘—æ¯”ä¾‹        |
  | å¸¸è¦‹ç¶­åº¦æ¯”ä¾‹  | $d_{\text{ff}} = 4 \times d$ï¼ˆæˆ– 2.7Ã—ï¼‰ |

#### ç”¨ä¸€å€‹ä¾‹å­è§£é‡‹ Attention + FFN, by ChatGPT 4o
* https://chatgpt.com/share/682dc278-486c-800c-b0e5-6dd121f2dfdb

2.2.2. Device-Limited Routing
* We design a device-limited routing mechanism to bound MoE-related communication costs.
* Due to the fine-grained expert segmentation in DeepSeekMoE, the number of activated experts can be large, so the MoE-related communication will be more costly if we apply expert parallelism.
* beyond the naive top-K selection of routed experts, we additionally ensure that the target experts of each token will be distributed on at most ğ‘€ devices
* To be specific
  * for each token, we first select ğ‘€ devices that have experts with the highest affinity scores in them
  * we perform top-K selection among experts on these ğ‘€ devices
  * In practice, we find that when $M \geq 3$, the device-limited routing can achieve a good performance roughly aligned with the unrestricted top-K routing

#### è«‹ ChatGPT 4o è§£é‡‹ Device-Limited Routing
https://chatgpt.com/share/682dc278-486c-800c-b0e5-6dd121f2dfdb
* èƒŒæ™¯ï¼šä»€éº¼æ˜¯ MoE çš„ expert routingï¼Ÿ
  * åœ¨ MoE æ¨¡å‹ä¸­ï¼Œæ¯å€‹ token ä¸¦ä¸æœƒç¶“éå…¨éƒ¨çš„ FFNï¼Œè€Œæ˜¯è¢«**è·¯ç”±ï¼ˆroutingï¼‰**åˆ°å°‘æ•¸å¹¾å€‹ã€Œå°ˆå®¶ï¼ˆexpert FFNï¼‰ã€ä¸­ï¼ˆä¾‹å¦‚ top-2 expertï¼‰ã€‚
* å•é¡Œï¼šç•¶ expert åˆ†å¸ƒåœ¨ä¸åŒ GPU æ™‚
  * å¦‚æœä¸€å€‹ token è¢«è·¯ç”±åˆ°çš„å…©å€‹ expert åˆ†å¸ƒåœ¨ ä¸åŒ GPUï¼Œé‚£é€™å€‹ token çš„ä¸­é–“è³‡æ–™å°±å¿…é ˆåœ¨ GPU é–“å‚³è¼¸ï¼Œé€šè¨Šæˆæœ¬æœƒè®Šé«˜ã€‚
  * è€Œ DeepSeek-V2 çš„ MoE çµæ§‹æ˜¯ ç´°ç²’åº¦åŠƒåˆ†ï¼ˆfine-grained expert segmentationï¼‰ï¼Œæ„å‘³è‘—ï¼š
    * expert æ•¸é‡è®Šå¤š
    * å¤šæ•¸ expert åˆ†å¸ƒåœ¨ä¸åŒè¨­å‚™
    * æ¯å€‹ token å¾ˆå¯èƒ½æœƒè¢«è·¯ç”±åˆ°è·¨å¥½å¹¾å¼µå¡ â†’ æˆæœ¬æ¿€å¢
* è§£æ³•ï¼šDevice-Limited Routing æ©Ÿåˆ¶
  * ä»–å€‘çš„è§£æ³•æ˜¯ï¼šå°æ¯å€‹ token é™åˆ¶å…¶ expert åªèƒ½ä¾†è‡ªæœ€å¤š M å¼µå¡ï¼ˆdeviceï¼‰ã€‚å…·é«”åšæ³•åˆ†å…©æ­¥ï¼š
    * Step 1: æŒ‘å‡º M å¼µ device
      * "we first select ğ‘€ devices that have experts with the highest affinity scores in them."
      * Affinity scoreï¼šæŒ‡ token èˆ‡è©² device ä¸Šçš„ expert çš„åŒ¹é…ç¨‹åº¦ï¼ˆä¾‹å¦‚ logitsï¼‰
      * æŒ‘å‡ºã€Œæœ€æœ‰å¯èƒ½æœ‰æ•ˆçš„ã€M å¼µå¡
    * Step 2: åœ¨é€™ M å¼µå¡ä¸Šçš„ expert è£¡é¸ top-K
      * "Then, we perform top-K selection among experts on these ğ‘€ devices."
      * æ‰€ä»¥ä¸æ˜¯åœ¨å…¨éƒ¨ expert è£¡é¸ top-Kï¼Œè€Œæ˜¯åœ¨ã€Œé€™ M å¼µå¡ä¸Šçš„ experts ä¸­é¸ top-Kã€
* å°çµ
  | å…ƒç´      | è§£é‡‹                                           |
  | ------ | -------------------------------------------- |
  | **å•é¡Œ** | expert åˆ†å¸ƒåœ¨å¤šå¼µ GPU æ™‚ï¼Œtoken è¦è·¨ device å‚³è¼¸ï¼Œé€ æˆå»¶é²   |
  | **ç›®æ¨™** | é™ä½ MoE çš„è·¨ device é€šè¨Šæˆæœ¬                        |
  | **åšæ³•** | é™åˆ¶æ¯å€‹ token çš„ expert åƒ…é¸è‡ªæœ€å¤š M å¼µå¡               |
  | **é¸æ³•** | å…ˆé¸ token èˆ‡å…¶æœ€ç›¸ç¬¦çš„ M å¼µå¡ â†’ å†åœ¨é€™äº›å¡ä¸Šé¸ top-K experts |
  | **æ•ˆæœ** | ç•¶ M â‰¥ 3 æ™‚ï¼Œæ•ˆèƒ½æ¥è¿‘åŸå§‹ top-K routingï¼Œä½†é€šè¨Šå¤§å¹…ä¸‹é™       |



## high-flyer
* https://www.high-flyer.cn/blog/llama2-1/
* https://github.com/deepseek-ai/DeepSeek-VL2


