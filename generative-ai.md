# Contents
  * Kubernetes (k8s)
  * ASAP - real2sim2real
  * DeepSeek
    * v2
  * high-flyer

## Kubernetes (k8s)
* Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation.
* Virtualized deployment era:
  * Each VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware.
* Container deployment era:
  * Containers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications.
    Therefore, containers are considered lightweight.
  * ä¸ä¼ ç»Ÿçš„è™šæ‹Ÿæœºä¸åŒï¼Œå®¹å™¨å…±äº«ä¸»æœºæ“ä½œç³»ç»Ÿçš„å†…æ ¸ï¼Œå› æ­¤æ›´åŠ é«˜æ•ˆå’Œä¾¿æºã€‚ 

## ASAP - real2sim2real
https://hao.cnyes.com/post/133793

## DeepSeek
### DeepSeek-VL2 - 2024 Jun 19
* https://github.com/deepseek-ai/DeepSeek-VL2
* a strong Mixture-of-Experts (MoE) language model
* 236B total parameters, 21B are activated for each token,  context length of 128K tokens
* innovative architectures
  * Multi-head Latent Attention (MLA)
    * MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector
  * DeepSeekMoE
    * enables training strong models at an economical cost through sparse computation
  * v.s. DeepSeek 67B:
    * significantly stronger performance
    * saves 42.5% of training costs
    * reduces the KV cache by 93.3%
    * boosts the maximum generation throughput to 5.76 times
  * pretrain: on a high-quality and multi-source corpus consisting of 8.1T tokens
  * Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)
![image](https://github.com/user-attachments/assets/bde50e68-af54-4627-8dd2-239be143dbe6)

1. Introduction
* we introduce MLA, an attention mechanism equipped with low-rank key-value joint compression.
  Empirically, MLA achieves superior performance compared with MHA, significantly reduces the KV cache during inference, thus boosting the inference efficiency.
* For Feed-Forward Networks (FFNs), we follow the DeepSeekMoE architecture (Dai et al., 2024), which adopts
  fine-grained expert segmentation and shared expert isolation for higher potential in expert specialization 
* We construct a high-quality and multi-source pre-training corpus consisting of 8.1T tokens.
* DeepSeek-V2 Chat (SFT): we collect 1.5M conversational sessions, which encompass various domains such as math, code, 
  writing, reasoning, safety, and more, to perform Supervised Fine-Tuning (SFT)  
* DeepSeek-V2 Chat (RL): Finally, we employ Group Relative Policy Optimization (GRPO) to further align the model with
  human preference
![image](https://github.com/user-attachments/assets/44477fe4-c33f-4cce-9cc4-40c9f1257ed8)
* We evaluate DeepSeek-V2 on a wide range of benchmarks in English and Chinese
  * even with only 21B activated parameters, V2 is the strongest open-source MoE language model
  * compared with DeepSeek 67B, V2 saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the
    maximum generation throughput to 5.76 times
  * V2 Chat (RL) achieves
    - 38.9 length-controlled win rate on AlpacaEval 2.0 (Dubois et al., 2024),
    - 8.97 overall score on MT-Bench (Zheng et al., 2023), and
    - 7.91 overall score on AlignBench (Liu et al., 2023).
  * DeepSeek-V2-Lite
    - 15.7B parameters
    - 2.4B are activated for each token.

2. Architecture
  * For attention, we design MLA, which utilizes low-rank key-value joint compression to
    eliminate the bottleneck of inference-time key-value cache, thus supporting efficient inference.
  * For Feed-Forward Networks (FFNs), we adopt the DeepSeekMoE architecture (Dai et al., 2024), a
    high-performance MoE architecture that enables training strong models at an economical cost.

2.1. Multi-Head Latent Attention: Boosting Inference Efficiency

2.1.1. Preliminaries: Standard Multi-Head Attention

#### Multi-Head Attention (MHA) è§£é‡‹ by DeepSeek V3:
* https://chat.deepseek.com/a/chat/s/e579d7f5-60ad-4fa2-87c3-88b37221b0a2

Multi-Head Attention (MHA) æ˜¯ Transformer æ¨¡å‹ä¸­çš„æ ¸å¿ƒæ©Ÿåˆ¶ï¼Œç”¨æ–¼æ•æ‰è¼¸å…¥åºåˆ—ä¸­ä¸åŒä½ç½®çš„é—œä¿‚ã€‚ä»¥ä¸‹æ˜¯å…¶é—œéµé»ï¼š
* Self-Attention åŸºç¤
  * é€šéè¨ˆç®—è¼¸å…¥åºåˆ—ä¸­æ¯å€‹å…ƒç´ èˆ‡å…¶ä»–å…ƒç´ çš„ç›¸é—œæ€§ï¼Œç”ŸæˆåŠ æ¬Šè¡¨ç¤ºã€‚
  * è¼¸å…¥åºåˆ— $$X = (x_1, x_2, \dots, x_n)$$\
    é€šéç·šæ€§è®Šæ›å¾—åˆ°æŸ¥è©¢ï¼ˆQueryï¼‰ã€éµï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰å‘é‡ï¼š\
    $$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$
  * æ³¨æ„åŠ›å¾—åˆ†é€šéé»ç©è¨ˆç®—ï¼š\
    $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$\
    å…¶ä¸­ $$\( d_k \)$$ æ˜¯éµå‘é‡çš„ç¶­åº¦ã€‚

* Multi-Head Attention
  - é€šéä¸¦è¡Œè¨ˆç®—å¤šå€‹ Self-Attention é ­ï¼Œæ•æ‰ä¸åŒçš„ç‰¹å¾µè¡¨ç¤ºã€‚
  - æ¯å€‹é ­ç¨ç«‹è¨ˆç®—æ³¨æ„åŠ›ï¼š\
    $head_i = \text{Attention}(QW_{Q_i}, KW_{K_i}, VW_{V_i})$

  - æ‰€æœ‰é ­çš„è¼¸å‡ºæ‹¼æ¥å¾Œé€šéç·šæ€§è®Šæ›ï¼š\
    $\text{MHA}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W_O$\
    $å…¶ä¸­ \ h \ æ˜¯é ­æ•¸ï¼Œ\ W_O \ æ˜¯è¼¸å‡ºæ¬Šé‡çŸ©é™£ã€‚$

* å„ªé»
  - **ä¸¦è¡Œè¨ˆç®—**ï¼šå¤šå€‹é ­å¯ä»¥åŒæ™‚è™•ç†ä¸åŒç‰¹å¾µã€‚
  - **å¤šæ¨£åŒ–ç‰¹å¾µæ•æ‰**ï¼šæ¯å€‹é ­é—œæ³¨ä¸åŒçš„å­ç©ºé–“ï¼Œå¢å¼·æ¨¡å‹è¡¨é”èƒ½åŠ›ã€‚

* æ‡‰ç”¨
  - **Transformer**ï¼šMHA æ˜¯å…¶ç·¨ç¢¼å™¨å’Œè§£ç¢¼å™¨çš„æ ¸å¿ƒã€‚
  - **BERTã€GPT**ï¼šé€™äº›é è¨“ç·´æ¨¡å‹ä¹Ÿä¾è³´ MHA é€²è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚

* ç¸½çµ
  - **Multi-Head Attention** é€šéä¸¦è¡Œè¨ˆç®—å¤šå€‹ Self-Attention é ­ï¼Œå¢å¼·äº†æ¨¡å‹æ•æ‰å¤šæ¨£åŒ–ç‰¹å¾µçš„èƒ½åŠ›ï¼Œæ˜¯ Transformer æ¨¡å‹æˆåŠŸçš„é—œéµã€‚

#### Multi-Head Attention (MHA) è§£é‡‹ by This paper 2.1.1:

$\text{Let } d \text{ be the embedding dimension, }$\
$n_h \text{ be the number of attention heads, }$\
$d_h \text{ be the dimension per head, and }$\
$h_t \in \mathbb{R}^d \text{ be the attention input of the ğ‘¡-th token at an attention layer.}$\
Standard MHA first produces $q_t, k_t, v_t \in \mathbb{R}^{d_h n_h}$ through three matrices $W^Q, W^K, W^V \in \mathbb{R}^{d_h n_h \times d}$
![image](https://github.com/user-attachments/assets/812a3f6e-4157-41e6-a204-f28eec731fdf)

![image](https://github.com/user-attachments/assets/8868b2d6-7093-4bcf-a353-211a75412d8d)
Figure 3 | Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through
jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV
cache during inference.

Then, $q_t, k_t, v_t$ will be sliced into $n_h$ heads for the multi-head attention computation:
![image](https://github.com/user-attachments/assets/37c9eb32-0668-4a36-9d21-b9e9730ba823)

* where $q_{t,i}, k_{t,i}, v_{t,i} \in \mathbb{R}^{d_h}$ denote the query, key, and value of the ğ‘–-th attention head, respectively;
* $W^o \in \mathbb{R}^{d \times d_h n_h}$ denotes the output projection matrix.
* During inference, all keys and values need to be cached to accelerate inference, so MHA needs to cache $2n_h d_h l$ elements for each token. 
* In model deployment, this heavy KV cache is a large bottleneck that limits the maximum batch size and sequence length.

2.1.2. Low-Rank Key-Value Joint Compression
* The core of MLA is the low-rank joint compression for keys and values to reduce KV cache:
![image](https://github.com/user-attachments/assets/19d3de7f-50ba-4d63-ad23-1e652a221c28)


3. 

## high-flyer
* https://www.high-flyer.cn/blog/llama2-1/
* https://github.com/deepseek-ai/DeepSeek-VL2


