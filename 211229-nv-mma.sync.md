Contents:
=========

### Fireiron: A Data-Movement-Aware Scheduling Language for GPUs
* https://lenary.co.uk/publications/fireiron.pdf
* PACT ’20, October 3–7, 2020, Virtual Event, USA
##### Tensor Core
* a warp is partitioned into four Quad-Pairs, groups of eight specific threads, which cooperatively execute an mma.sync instruction to compute an 8×8 tile of the output. 

* WMMA: exposes a conceptually single, warp-wide macro-mma using a fixed data-to-quad-pair mapping that is optimal in some cases but not all. e.g. *a quad-pair, operate on interleaved distributed tiles.

### References
* High Performance GPU Tensor Core Code Generation for Matmul Using MLIR (2021)
  https://mlir.llvm.org/OpenMeetings/2021-08-26-High-Performance-GPU-Tensor-CoreCode-Generation-for-Matmul-Using-MLIR.pdf

* PROGRAMMING TENSOR CORES: NATIVE VOLTA TENSOR CORES WITH CUTLASS (2019)
  https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9593-cutensor-high-performance-tensor-operations-in-cuda-v2.pdf

* CUTLASS GEMM
  https://github.com/NVIDIA/cutlass/blob/master/media/docs/gemm_api.md

* cutlass/arch/mma_sm80.h
  https://github.com/NVIDIA/cutlass/blob/master/include/cutlass/arch/mma_sm80.h

* Matrix multiply-accumulate operation using mma instruction
  https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-for-mma

* Matrix Fragments for WMMA
  https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment

* Matrix Shape
  https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-shape
  
  
